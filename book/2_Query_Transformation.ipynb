{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation\n",
    "\n",
    "The main idea behind the Query Transformation is that translate/transform the user query in a way that the LLM can correctly answer the question. For instance, if the user asks an ambiguous question, our RAG retriever might retrieve incorrect (or ambiguous) documents based on the embeddings that are not very relevant to answer the user question, leading the LLM to hallucinate answers. There are few ways to tackle this problem. Some of them are,\n",
    "\n",
    "- [Step-back prompting](https://arxiv.org/pdf/2310.06117): This involves encouraging the LLM to take a step back from a given question or problem and pose a more abstract, higher-level question that encompasses the essence of the original inquiry.\n",
    "- [Least-to-most prompting](https://arxiv.org/pdf/2205.10625): This allows to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems.\n",
    "- Query re-writing ([Multi-Query](https://medium.com/@kbdhunga/advanced-rag-multi-query-retriever-approach-ad8cd0ea0f5b) or [RAG Fusion](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)): This allows to generate multiple questions from the original question with different wording and perspectives. Then retrieve documents using the similarity scores between each question and the vector store to answer the orginal question.\n",
    "\n",
    "A blog post about query transformation by Langchain can be found [here](https://blog.langchain.dev/query-transformations/). \n",
    "\n",
    "Now, let's try to implement the above techniques using LangChain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Introduction notebook, we first import the libraries, load documents, split them, generate embeddings, store them in a vector store and create the retriever using the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.load import loads, dumps\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('data/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query\n",
    "\n",
    "In multi-query approach, we first use an LLM (here it is an instance of GPT-4) to generate 5 different questions based on our original question. To do that, we create a prompt and encapsulate it with the `ChatPromptTemplate`. Then we create the chain using LCEL, to read the user input and assign it to the `question` placeholder of the prompt, send the prompt to the LLM, parse the output containing 5 questions seperated by new line charcters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent assistant. Your task is to generate 5 questions based on the provided question in different wording and different perspectives to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether or not our query generation works by invoking the created chain with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Can you explain the advantages of using QLoRA?',\n",
       " '2. What positive impacts does QLoRA have?',\n",
       " '3. Could you describe the beneficial aspects of QLoRA?',\n",
       " '4. What are the reasons to choose QLoRA for a project?',\n",
       " '5. What are the beneficial features of QLoRA?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries.invoke(\"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the 5 questions, we parallelly retrieve the most relevant 5 documents for each question (resulting in a list of lists) and create a new document list by taking the unique documents of the union of all the retrieved documents. To do that we create another chain, `retrieval_chain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_union(docs: List[List]):\n",
    "    all_docs = [dumps(d) for doc in docs for d in doc]\n",
    "    unique_docs = list(set(all_docs))\n",
    "    \n",
    "    return [loads(doc).page_content for doc in unique_docs] # We only return page contents\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | get_context_union\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the',\n",
       " 'There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoRA is far from clear – how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-\\n12',\n",
       " 'Quantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA',\n",
       " 'technology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis',\n",
       " 'trade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put all together by creating a one final chain to read the user query, get the contexts from 5 different documents using the `retrieval_chain`, add both the question and context to the prompt, send it through the LLM, and get the final formatted output using  the `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "multi_query_chain = (\n",
    "    {'context': retrieval_chain, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The benefits of QLoRA include reducing the average memory footprint by quantizing the quantization constants and managing memory spikes through Paged Optimizers. It allows for the fine-tuning of more than 1,000 models and provides a detailed analysis of instruction following and chatbot performance across multiple datasets, model types, and model scales. QLoRA can also be seen as an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs. Another potential benefit is its deployment to mobile phones, enabling the fine-tuning of LLMs on phones and other low resource settings.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_chain.invoke(\"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing all the above cells, you will be able to see a LangSmith trace like [this](https://smith.langchain.com/public/31d1e43a-3727-4d0b-82fb-2bbdf146dfac/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Fusion\n",
    "\n",
    "In the default multi-query approach, after we retrieved the relevant documents for each question generated for our original question, we take the union of all the documents to select only unique documents (same document can be retrieved by multiple questions). However, we did not pay attention to the rank of each docuemnt in the context, which is important for the LLM to produce the most correct answer. Beacuse the each individual rank would help us to decide the top-k documents to select as the context if we have a huge number of documents with a limited context window of the LLM. Therefore in RAG Fusion, while we do exactly the same thing upto retrieving docuemnts, we use [Reciprocal Rank Fusion (RRF)](https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking) to rank the each retrieved document before using them as the context to answer our original question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(results: List[List], k=60):\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between the below code compared to the multi-query code we went through earlier is, now we use our `rrf` method instead of `get_context_union` to retrieve the final list of documents related to our original question (i.e., context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent assistant. Your task is to generate 4 questions based on the provided question in different wording and different perspectives to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "fusion_retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | rrf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Quantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA', metadata={'page': 0, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.12987629896837988),\n",
       " (Document(page_content='technology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis', metadata={'page': 15, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.08068715742069787),\n",
       " (Document(page_content='All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the', metadata={'page': 15, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.049189141547682),\n",
       " (Document(page_content='trade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models', metadata={'page': 6, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.03149801587301587),\n",
       " (Document(page_content='There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoRA is far from clear – how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-\\n12', metadata={'page': 11, 'source': 'data/LoRA.pdf'}),\n",
       "  0.03149801587301587)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusion_retrieval_chain.invoke(\"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we format the context by considering only the page contents without meta data or re-ranking scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(documents: List):\n",
    "    return \"\\n\\n\".join([doc[0].page_content for doc in documents])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "multi_query_chain = (\n",
    "    {'context': fusion_retrieval_chain | format_context, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLoRA offers several benefits. It serves as an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs. It also has the potential to enable the finetuning of large language models (LLMs) on phones and other low resource settings. QLoRA can reduce the average memory footprint by quantizing the quantization constants and manage memory spikes. It has been used to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across multiple datasets and model types. It makes the finetuning of high quality LLMs much more widely and easily accessible. Furthermore, QLoRA matches 16-bit performance across scales, tasks, and datasets.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_chain.invoke(\"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing all the above cells, you will be able to see a LangSmith trace like [this](https://smith.langchain.com/public/99c5fb68-0ccf-4508-a72d-7c3a7b5e61d2/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Decomposition\n",
    "\n",
    "In \"Query Translation\", we focused on generating multiple questions from our original question with different perspectives (i.e., translate the query) to improve RAG. However, the generated questions all do have the same meaning despite the wording is different, since it is in fact translation. Therefore, the answers for all the questions are somewhat similar. As a result, while the multi-query approach helps avoid ambiguities of the user query by writing it in different ways, it will not help when the user query is complex (e.g., a long mathematical computation).\n",
    "\n",
    "As a solution we can break down (i.e., decompose) the original query into multiple sub-problems (like in recursion or dynamic programming) and answer each sub-problem sequentially/parallelly to derive the answer to our original query. This simplifies the prompts and increases the context for the retrieval process. We do that using \"Query Decomposition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-to-Most Prompting\n",
    "\n",
    "First let's look at how to implement [Least-to-Most Prompting](https://arxiv.org/pdf/2205.10625) to break down a complex query into subquestions and answer them recursively to derive the final answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the multi-query and RAG fusion we first have generate a few questions based on our original questions. However our prompt should be different as we are generating sub questions by decomposing the original one, instead of generating the same question with different perspectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "decompostion_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant that can break down complex questions into simpler parts. \\n\n",
    "        Your goal is to decompose the given question into multiple sub-questions that can be answerd in isolation to answer the main question in the end. \\n\n",
    "        Provide these sub-questions separated by the newline character. \\n\n",
    "        Original question: {question}\\n\n",
    "        Output (3 queries): \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_generation_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | decompostion_prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is QLoRA?',\n",
       " 'What are the features of QLoRA?',\n",
       " 'How do these features provide benefits?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = query_generation_chain.invoke(\"What are the benefits of QLoRA?\")\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the sub-questions, we iterate through them to answer them individually using the `least_to_most_chain`. We first extract the `question` from the user input using the `itemgetter` and provide it to our `retriever` to retrieve related documents as the `context`. `q_a_pairs` will also be provided as part of the user input. Then we populate our prompt and send to the LLM to get the answer. Each time we store the sub-question `Q_{n-1}` and its answer `A_{n-1}` since we provide them as the context to answer the question `Q_{n}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# Create the final prompt template to answer the question with provided context and background Q&A pairs\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "least_to_most_prompt = ChatPromptTemplate.from_template(template) \n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "\n",
    "least_to_most_chain = (\n",
    "        {'context': itemgetter('question') | retriever,\n",
    "        'q_a_pairs': itemgetter('q_a_pairs'),\n",
    "        'question': itemgetter('question'),\n",
    "        }\n",
    "        | least_to_most_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    answer = least_to_most_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pairs+=f\"Question: {q}\\n\\nAnswer: {answer}\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting answers for the 3 generated sub-questions, finally we answer our original question by invoking the `least_to_most_chain` once again, but this time with the original question and all `q_a_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The benefits of QLoRA include:\\n\\n1. Reduced Memory Footprint: QLoRA uses quantization to reduce the average memory footprint, allowing for more efficient use of memory resources. This is particularly beneficial when working with large models that require significant memory.\\n\\n2. Management of Memory Spikes: QLoRA employs Paged Optimizers to manage memory spikes, ensuring smooth operation even with large models. This enhances the stability and reliability of the model training process.\\n\\n3. Detailed Analysis: QLoRA has been used to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across multiple datasets and model types. This allows for a comprehensive understanding of model performance and can guide further improvements.\\n\\n4. Handling Large Model Scales: QLoRA can handle model scales that would be infeasible to run with regular finetuning, such as 33B and 65B parameter models. This expands the range of models that can be finetuned, opening up new possibilities for model development.\\n\\n5. Equalizing Resource Gap: QLoRA can be seen as an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs. This democratizes access to advanced model finetuning, fostering innovation and competition.\\n\\n6. Enabling Finetuning on Low Resource Settings: QLoRA has the potential to enable the finetuning of large language models on phones and other low resource settings. This makes advanced model finetuning more accessible and versatile, potentially expanding the range of applications for these models.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_to_most_chain.invoke({\"question\": \"What are the benefits of QLoRA?\", \"q_a_pairs\": q_a_pairs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the original question answer will look like [this](https://smith.langchain.com/public/7bd7f987-a53a-4d32-abb0-823940bc3f27/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead sequentially answering the sub-questions, we can use the LLM to answer them parallely and use those answers to derive the final answer to our main question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_answer(question):\n",
    "    \n",
    "    questions = []\n",
    "    \n",
    "    sub_questions = query_generation_chain.invoke(question)\n",
    "    \n",
    "    sub_qa_chain = (\n",
    "        {'context': RunnablePassthrough() | retriever, 'question': RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    for q in sub_questions:\n",
    "        answer = sub_qa_chain.invoke(q)\n",
    "        questions.append({\"question\": q, \"answer\": answer})\n",
    "        \n",
    "    return questions\n",
    "        \n",
    "qa_pairs = generate_and_answer(\"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The benefits of QLoRA include its ability to potentially combine with other efficient adaptation methods for orthogonal improvement, making it more tractable to understand how features learned during pre-training are transformed to perform well on downstream tasks. It also helps close the resource gap between large corporations and small teams with consumer GPUs. Furthermore, QLoRA may enable the fine-tuning of large language models on phones and other low resource settings.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(qa_pairs):\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    \n",
    "    for i, qa in enumerate(qa_pairs):\n",
    "        formatted_string += f\"Question {i}: {qa['question']}\\nAnswer {i}: {qa['answer']}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(qa_pairs)\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Consider the following Question and Answer Pairs:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Use these to synthesize an answer to the question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "final_rag_chain = (\n",
    "     prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'context': context, 'question': \"What are the benefits of QLoRA?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for answering the original question will look like [this](https://smith.langchain.com/public/d5a17200-7752-42cb-87b9-146959e691bc/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step back prompting\n",
    "\n",
    "[Step back prompting](https://arxiv.org/pdf/2310.06117) allows LLMs to step back through in-context learning – prompting them to derive high-level abstractions such as concepts and principles for a specific example (i.e., Abstraction). Then, grounded on the documents regarding the high-level concept or principle, the LLM can reason about the solution to the original question (i.e., Reasoning).\n",
    "\n",
    "E.g., If the original question is \"What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?\", a possible step-back question would be \"What are the physics principles behind this question?\". Then the context (i.e., documents) retrieved for the step-back question will be used as additional context to answer the original question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate such step-back questions, we use few-shot learning to provide a few examples of (question, step-back question) pairs to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\\nHuman: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?\\nAI: What are the physics principles behind this question?\\nHuman: Estella Leopold went to which school between Aug 1954 and Nov 1954?\\nAI: What was Estella Leopold's education history?\\nHuman: What are the benefits of QLoRA?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        'input': 'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?',\n",
    "        'output': 'What are the physics principles behind this question?'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Estella Leopold went to which school between Aug 1954 and Nov 1954?',\n",
    "        'output': \"What was Estella Leopold's education history?\"\n",
    "    }\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                ('human', '{input}'), ('ai', '{output}')\n",
    "            ]\n",
    "        )\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "            # This is a prompt template used to format each individual example.\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                ('system', \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\"),\n",
    "                few_shot_prompt,\n",
    "                ('user', '{question}'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "final_prompt.format(question= \"What are the benefits of QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the created few-shot prompt to generate the step-back question through a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the factors to consider when setting parameters for QLoRA?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_query_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | final_prompt \n",
    "    | ChatOpenAI(model='gpt-4', temperature=0.7) \n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "step_back_query_chain.invoke(\"What are the optimal parameters for QLoRA?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use both the context retrieved for the original question and the context retrieved for the step-back question to answer our original question via the `step_back_chain`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "step_back_chain = (\n",
    "    {'normal_context': RunnablePassthrough() | retriever,\n",
    "     'step_back_context': RunnablePassthrough() | step_back_query_chain | retriever,\n",
    "     'question': RunnablePassthrough()\n",
    "     }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document does not provide specific optimal parameters for QLoRA. However, it mentions that a hyperparameter search is conducted for LoRA over the following variables: LoRA dropout with values { 0.0, 0.05, 0.1}, LoRA r with values { 8, 16, 32, 64, 128, 256}, and LoRA layers which include {key+query, all attention layers, all FFN layers, all layers, attention + FFN output layers}. The LoRA α is kept fixed and the learning rate is searched, as LoRA α is always proportional to the learning rate.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_chain.invoke(\"What are the optimal parameters for QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the implemented step-back prompting chain will look like [this](https://smith.langchain.com/public/425c098b-47ae-4f53-9259-8cd6b567a2b0/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we looked at ways to improve the LLMs answers to a user query through the \"Query Transformation\". In summary, query transformation may help us to remove ambiguities of the user query and simplify it through techniques such as ,\n",
    "\n",
    "- **Multi-query**: That re-writes the question in different perspectives (i.e., sub-questions).\n",
    "- **RAG Fusion**: That not only re-writes the question in different perspectives, but also rank the documents retrieved for each sub-question to provide the most relevant information to answer the original question.\n",
    "\n",
    "- **Least-to-Most Prompting**: That helps break-down complex questions into mutiple sub problems and answer the final question using the sub problems and their answers as the context.\n",
    "- **Step-back Prompting**: That generates a step-back question and use the retrieved documents for that step-back question as the additional context to answer the original question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will  generate Hypothetical Documents, instead of questions to help LLMs answer questions more accurately through [HyDE](https://arxiv.org/pdf/2212.10496) (Hypothetical Document Embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
