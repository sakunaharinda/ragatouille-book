{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "When we have multiple data sources such as a GraphDB, PDF documents (i.e., a vector store), we might need to answer user queries based on the correct data source. For example, if the user wants to know about reviews of a hospital, user query should be redirected to the vector store containing embeddings of hospital reviews. On the other hand, if the user wants to know about information such as the doctors, patients, their visits to the hospital, the user query should probably be send to a graph database that contains the hospial information. Therefore, to provide such as functionality we will now focus on \"Routing\" in RAG with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we talk about two main types of routing techniques, namely **Logical routing** and **Semantic routing**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import our libraries and create two vector stores to where we re-direct the user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakunaharinda/Documents/Repositories/ragatouille/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def generate_vectorstores(file, dir):\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split text into chunks\n",
    "\n",
    "    text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                        embedding=OpenAIEmbeddings(),\n",
    "                                        persist_directory=dir)\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# Create a vectorstore to answer questions about LoRA\n",
    "vectorstore_lora = generate_vectorstores(\"data/LoRA.pdf\",\"data/vectorstore_lora\")\n",
    "\n",
    "# Create a vectorstore to answer questions about BERT\n",
    "vectorstore_bert = generate_vectorstores(\"data/BERT.pdf\",\"data/vectorstore_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_lora = vectorstore_lora.as_retriever(search_kwargs={'k':5})\n",
    "retriever_bert = vectorstore_bert.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Routing\n",
    "\n",
    "In logical routing we let the LLM to decide the route based on a set of pre-defined options/routes. To do that we first define our router with three main routes as a Pydantic [model](https://docs.pydantic.dev/latest/concepts/models/). In the `QueryRouter` model, we define 2 fields, namely `datasource` indicating the datasource where the query is re-directed to and the `question` representing the user query. For the `datasource` field, we allow three values \"lora\", \"bert\" that represent two vectore stores we created earlier, and \"general\" to route the query directly to the LLM as the fallback mechanism.\n",
    "\n",
    "After specifying our router we initialize our LLM as GPT-4 to provide the output as a `QueryRouter` object using `with_structured_output()` method. \n",
    "\n",
    "Finally, we crate our router chain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "\n",
    "class QueryRouter(BaseModel):\n",
    "    \n",
    "    \"\"\"Route a user query to the appropriate datasource that will help answer the query accurately\"\"\"\n",
    "    \n",
    "    datasource: Literal['lora', 'bert', 'general'] = Field(..., \n",
    "                                                description=\"Given a user question choose which datasource would be most relevant for answering their question\"\n",
    "                                                )\n",
    "    question: str = Field(..., description=\"User question to be routed to the appropriate datasource\")\n",
    "    \n",
    "llm = ChatOpenAI(model='gpt-4',temperature=0)\n",
    "structured_llm = llm.with_structured_output(QueryRouter)\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert router that can direct user queries to the appropriate datasource. Route the following user question about a topic in NLP and LLMs to the appropriate datasource.\\nIf it is a general question not related to the provided datasources, route it to the general datasource.\\n\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | router_prompt\n",
    "    | structured_llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After invoking our router chain we will be able to see it logically decides the datasource to redirect the query and output it as a `QueryRouter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryRouter(datasource='bert', question='How does the BERT work?')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How does the BERT work?\"\n",
    "result = router.invoke(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to use the router output and perform the QA accordingly, we define a new method `choose_route`. `choose_route` checks the router chain result to extract the datasource and defines three chains to answer the questions related to BERT, LoRA, and general domain. \n",
    "\n",
    "We complete our RAG with one final chain by putting all the methods and chains together in the `full_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "\n",
    "def choose_route(result):\n",
    "    \n",
    "    llm_route = ChatOpenAI(model='gpt-4',temperature=0)\n",
    "    if \"bert\" in result.datasource.lower():\n",
    "        print(f\"> Asking about BERT ...\\nQuestion: {result.question}\\nAnswer:\")\n",
    "        bert_chain = (\n",
    "            {'context': retriever_bert, 'question': RunnablePassthrough()}\n",
    "            | qa_prompt\n",
    "            | llm_route\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return bert_chain.invoke(result.question)\n",
    "    elif \"lora\" in result.datasource.lower():\n",
    "        print(f\"> Asking about LoRA ...\\nQuestion: {result.question}\\nAnswer:\")\n",
    "        lora_chain = (\n",
    "            {'context': retriever_lora, 'question': RunnablePassthrough()}\n",
    "            | qa_prompt\n",
    "            | llm_route\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return lora_chain.invoke(result.question)\n",
    "    else:\n",
    "        print(f\"> Asking about a general question ...\\nQuestion: {result.question}\\nAnswer:\")\n",
    "        general_chain = llm_route | StrOutputParser()\n",
    "        return general_chain.invoke(result.question)\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Asking about LoRA ...\n",
      "Question: What are the benefits of LoRA?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"LoRA, or Low-Rank Adaptation, offers several benefits. It makes training more efficient and lowers the hardware barrier to entry by up to three times, as it doesn't require the calculation of gradients or maintenance of optimizer states for most parameters. It also allows for quick task-switching when deployed as a service by sharing the majority of the model parameters, and it reduces the number of trainable parameters and the GPU memory requirement, without introducing additional inference latency.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke(\"What are the benefits of LoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for our logical router will look like [this](https://smith.langchain.com/public/8a9330a0-5254-4602-8c26-5c9baa158eb5/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Rounting\n",
    "\n",
    "In contrast to the logical routing, semantic routing depends on the semantic similarity between the user query and the router prompts to decide which route to take. Let's try to implement it for RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define two prompts representing the two routes of our semantic router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{question}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{question}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly we generate embedding vectors for both of those prompts using `OpenAIEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "routes = [physics_template, math_template]\n",
    "route_embeddings = embeddings.embed_documents(routes)\n",
    "len(route_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the router that first embed the user query and get the cosine similarity scores between the query embeddings and the embeddings of each prompt. Depending on the similarity, the router returns the prompt that has the highest similarity with the query to use as the prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def router(input):\n",
    "    # Generate embeddings for the user query\n",
    "    query_embedding = embeddings.embed_query(input['question'])\n",
    "    # Getting similarity scores between the user query and the routes. This contains the similarity scores between the user query and each of the two routes.\n",
    "    similarity = cosine_similarity([query_embedding], route_embeddings)[0]\n",
    "    # Find the route that gives the maximum similarity score\n",
    "    route_id = similarity.argmax()\n",
    "    if route_id == 0:\n",
    "        print(f\"> Asking a physics question ...\\nQuestion: {input['question']}\\nAnswer:\")\n",
    "    else:\n",
    "        print(f\"> Asking a math question ...\\nQuestion: {input['question']}\\nAnswer:\")\n",
    "        \n",
    "    return PromptTemplate.from_template(routes[route_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our RAG chain that first takes the user query and then answer it using the appropriate prompt decided by the router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Asking a math question ...\n",
      "Question: What is the formula for the area of a circle?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The formula for the area of a circle is A = πr², where A is the area and r is the radius of the circle.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_router_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | RunnableLambda(router)\n",
    "    | ChatOpenAI(model='gpt-4',temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "semantic_router_chain.invoke(\"What is the formula for the area of a circle?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is much more simpler compared to the logical routing. The LangSmith trace for our semantic router will look like [this](https://smith.langchain.com/public/7e2d7c78-7e70-48d4-bdf2-4c16e161d6b3/r)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
