{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "Instead of generating queries based on the original question, [HyDE](https://arxiv.org/pdf/2212.10496) focuses on generating hypothetical docuemnts for a given query. The intution behind generating such hypothetical documents is their embedding vectors can be used to identify a neighborhood in the corpus embedding space where similar real documents are retrieved based on vector similarity. In that case, RAG will be able to retrieve more relevant documents based on the hypothetical documents to answer the user query accurately. \n",
    "\n",
    "Let's try to use HyDE to answer questions through RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, similar to the previous notebooks, first we create our vector store and initialize the retriever using `OpenAIEmbeddings` and `Chroma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakunaharinda/Documents/Repositories/ragatouille/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('data/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we ask the LLM to write a \"hypothetical\" passage on the asked question through a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please write a scientific passage of a paper to answer the following question:\\n\n",
    "    Question: {question}\\n\n",
    "    Passage: \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_doc_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | hyde_prompt\n",
    "    | ChatOpenAI(model='gpt-4',temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Low Rank Adapters (LRAs) are a recent development in the field of Large Language Models (LLMs) that aim to reduce the computational and memory requirements of these models while maintaining their performance. The fundamental principle behind LRAs is the use of low-rank approximations to reduce the dimensionality of the model's parameters.\\n\\nIn the context of LLMs, an adapter is a small neural network that is inserted between the layers of a pre-trained model. The purpose of this adapter is to adapt the pre-trained model to a new task without modifying the original parameters of the model. This allows for efficient transfer learning, as the pre-trained model can be adapted to a wide range of tasks with minimal computational cost.\\n\\nLow Rank Adapters take this concept a step further by using low-rank approximations to reduce the number of parameters in the adapter. This is achieved by decomposing the weight matrix of the adapter into two low-rank matrices. The resulting model has fewer parameters and thus requires less memory and computational resources.\\n\\nDespite the reduction in parameters, LRAs have been shown to maintain the performance of the original model. This is because the low-rank approximation captures the most important features of the data, and these features are sufficient to achieve high performance on the task.\\n\\nIn conclusion, Low Rank Adapters work in LLMs by using low-rank approximations to reduce the number of parameters in the model. This allows for efficient transfer learning and reduces the computational and memory requirements of the model. Despite the reduction in parameters, LRAs maintain the performance of the original model, making them a promising approach for large-scale language modeling tasks.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How Low Rank Adapters work in LLMs?\"\n",
    "generate_doc_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the generated passage, we then retrieve the similar documents using our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3', metadata={'page': 1, 'source': 'data/LoRA.pdf'}),\n",
       " Document(page_content='over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3', metadata={'page': 1, 'source': 'data/LoRA.pdf'}),\n",
       " Document(page_content='over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3', metadata={'page': 1, 'source': 'data/LoRA.pdf'}),\n",
       " Document(page_content='over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3', metadata={'page': 1, 'source': 'data/LoRA.pdf'}),\n",
       " Document(page_content='over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3', metadata={'page': 1, 'source': 'data/LoRA.pdf'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain = generate_doc_chain | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the retrieved docuemnts based on the \"hypothetical\" passage is used as the context to answer our original question through the `final_rag_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Low-Rank Adapters (LoRA) work in Large Language Models (LLMs) by allowing the training of some dense layers in a neural network indirectly. This is done by optimizing rank decomposition matrices of the dense layers\\' change during adaptation, while keeping the pre-trained weights frozen. The hypothesis is that the change in weights during model adaptation has a low \"intrinsic rank\".'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on the provided context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(model='gpt-4',temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this technique might help answer questions, there is a chance to the answer be wrong due to retrieving documents based on the incorrect/hallucinated hypothetical passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we talk about \"Routing\" in RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
