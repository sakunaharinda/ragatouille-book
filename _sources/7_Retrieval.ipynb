{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "The basic RAG pipeline involves embedding a user query, retrieving relevant documents to the query, and passing the documents to an LLM for generation of an answer grounded in the retrieved context as we seen through out this tutorial series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When retrieving documents we usually rerank them based on a certain algorithm like Reciprocal Rank Fusion (RRF). Instead of providing all the documents retrieved as the context, reranking allows us to provide only top-k relevant documents saving the limited context length of most affordable LLMs. \n",
    "\n",
    "In this section we implement two reranking-based retrieval methods, namely,\n",
    "\n",
    "- Reciprocal Rank Fusion (RRF)\n",
    "- Cohere Reranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from typing import List\n",
    "from langchain.load import loads, dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakunaharinda/Documents/Repositories/ragatouille/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('data/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocal Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(results: List[List], k=60):\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Quantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA', metadata={'page': 0, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.06612021857923497),\n",
       " (Document(page_content='A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.', metadata={'page': 21, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.06535177595628415),\n",
       " (Document(page_content='technology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis', metadata={'page': 15, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.06452452301209573),\n",
       " (Document(page_content='trade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models', metadata={'page': 6, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.04712301587301587),\n",
       " (Document(page_content='LoRA, an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input\\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\\non Transformer language models, the proposed principles are generally applicable to any neural\\nnetworks with dense layers.', metadata={'page': 11, 'source': 'data/LoRA.pdf'}),\n",
       "  0.03200204813108039),\n",
       " (Document(page_content='All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the', metadata={'page': 15, 'source': 'data/QLoRA.pdf'}),\n",
       "  0.016129032258064516),\n",
       " (Document(page_content='There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoRA is far from clear – how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-\\n12', metadata={'page': 11, 'source': 'data/LoRA.pdf'}),\n",
       "  0.015873015873015872),\n",
       " (Document(page_content='construction.\\n• LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preﬁx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the', metadata={'page': 1, 'source': 'data/LoRA.pdf'}),\n",
       "  0.015625)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "question = \"What is QLoRA?\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent assistant. Your task is to generate 4 questions based on the provided question in different wording and different perspectives to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "fusion_retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | rrf\n",
    ")\n",
    "\n",
    "fusion_retrieval_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLoRA is a technology used to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across multiple datasets and model types. It is an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs. It might enable the critical milestone of enabling the finetuning of Large Language Models (LLMs) on phones and other low resource settings. QLoRA matches 16-bit performance across scales, tasks, and datasets. It is an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_context(documents: List):\n",
    "    return \"\\n\\n\".join([doc[0].page_content for doc in documents])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rag_with_rrf_chain = (\n",
    "    {'context': fusion_retrieval_chain | format_context, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_with_rrf_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere Reranking\n",
    "\n",
    "| ![Cohere](resources/cohere.png) | \n",
    "|:--:| \n",
    "| *[Cohere Reranking](https://cohere.com/blog/rerank) process* |\n",
    "\n",
    "Cohere uses a tranformer model to rerank relevant documents per a user’s query. This means that companies can retain an existing keyword-based (also called \"lexical\") or semantic search system for the first-stage retrieval and integrate the Rerank endpoint in the second stage re-ranking. Since this technique reranks the documents based on their content instead of their frquecy in the retrieved document set (as seen in RRF), this is much more accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} See also\n",
    ":class: tip\n",
    "Their [blog post](https://cohere.com/blog/rerank) explains this process in detail.  \n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we define the Cohere Reranker with the model `rerank-english-v2.0` to retrieve top 3 documents for the given question. Then we pass the reranking model together with our base retriever that provides the initial document set to rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Instead of immediately returning retrieved documents as-is, `ContextualCompressionRetriever` compresses them using the context of the given query, so that only the relevant information is returned. \"Compressing\" here refers to both compressing the contents of an individual document and filtering out documents wholesale. Refer the [documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/contextual_compression/) for more information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakunaharinda/Documents/Repositories/ragatouille/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Quantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA', metadata={'page': 0, 'source': 'data/QLoRA.pdf', 'relevance_score': 0.998259}),\n",
       " Document(page_content='technology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis', metadata={'page': 15, 'source': 'data/QLoRA.pdf', 'relevance_score': 0.9972316}),\n",
       " Document(page_content='A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.', metadata={'page': 21, 'source': 'data/QLoRA.pdf', 'relevance_score': 0.98875546})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain.retrievers import  ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v2.0\", top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLoRA is a technology used to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across multiple instruction datasets, model types, and model scales. It uses quantization to reduce the average memory footprint and Paged Optimizers to manage memory spikes. It is seen as an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs. It might also enable the critical milestone of enabling the finetuning of LLMs on phones and other low resource settings.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "cohere_with_rag_chain = (\n",
    "    {'context': compression_retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "cohere_with_rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the Cohere reranking will look like [this](https://smith.langchain.com/public/3c338204-1db7-4ef7-8528-bbde7cc3125c/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
